---
title: "Data Example with NABTC Data"
author: "A, A, & R"
date: "11/15/2016"
output: html_document
---


```{r}
                            
aim2 <- function(dat,nreps=1,n.grid=20,mult=2,seed=12345,outvar="Y")  
{
  #Functions that go into penalized fitting method  
  aim2.list <- list(eval=aim2.eval, split=aim2.split, init=aim2.init, 
                    summary=aim2.summary, text=aim2.text)
  set.seed(seed)
  n <- nrow(dat)
  
  #Identify outcome variable   --- this is redundant with functions statement and should be changed
  which.outcome <- which(colnames(dat)==outvar)
  colnames(dat)[which.outcome] <- "outvar.aim2"
  
  #Split training set into learning and evaluation sets - now based on 50/50 split
  # later look at different alternatives
  nlearn <- round(n/2)
  neval <- n-nlearn
  samp <- sample(1:n,n,replace=FALSE)
  wlearn <- sort(samp[1:nlearn])
  weval <- sort(samp[(nlearn+1):n])
  learning.dat <- dat[wlearn,]
  evaluation.dat <- dat[weval,]

  #The lambdas chosen using a grid. Here, get values for variables based on root node
  fit.rf.learning <- randomForest(outvar.aim2 ~ .,data = learning.dat) # Fit RF with learning set
  predict.rf.evaluation <- predict(fit.rf.learning,newdata=evaluation.dat,
                                   predict.all=TRUE) #  $\widehat{Z}_{1i}$
  mean.evaluation <- mean(evaluation.dat$outvar.aim2) # $\mu_{Z_1}$
  var.evaluation <- var(evaluation.dat$outvar.aim2) # $\sigma^2_{Z_1}$
  zbarhat <- mean(predict.rf.evaluation$aggregate) # $ \bar{\hat{Z_1}}$ 
  
    # NOTE: this zbarhat means we are doing the 
    # alternative shrinkage target in equation 9 not the original in 6
  
  var.z1s <-  apply(predict.rf.evaluation$individual,1,var) # $\sigma^2_{\hat{Z_1i}}$
  alphas <- 1/var.z1s # $\alpha_i$
  alphabar <- mean(alphas) # \bar{\alpha}$
  lambda <- var.evaluation/(neval*alphabar*(mean.evaluation-zbarhat)^2) #with-in node 
                                                                        #choice of \lambda
  lambdas <- seq(0,mult*lambda,length.out=n.grid)  # list of possible lambdas
  n.lambdas <- length(lambdas) #length of list
  error.lambdas <- rep(0,length(lambdas)) 
  fits <- vector("list",n.lambdas)
  predictions <- vector("list",n.lambdas)
  
  # To get the err(\lambda) - uncorrected - currently equation 11
  for(j in 1:n.lambdas)
    {
      current.fit <- rpart(outvar.aim2 ~ .,data = evaluation.dat,
                           parms=list(lambda=lambdas[j],
                           yhat=predict.rf.evaluation$aggregate,
                           alpha=alphas),method=aim2.list)   
      predicted.fit <- predict(object=current.fit,newdata=evaluation.dat)
      error.lambdas[j] <- sum((evaluation.dat$outvar.aim2-predicted.fit)^2)
      fits[[j]] <- current.fit
      predictions[[j]] <- predicted.fit
  }
  
  # To get the optimism for correcting the err(\lambda)
  optimism <- corrected.lambda(dat=evaluation.dat,lambdas=lambdas,
                               list.object=aim2.list,model=fit.rf.learning,
                               predicted.values=predict.rf.evaluation$aggregate,
                               alphas=alphas,n.boot=10)
  
  # err(\lambda) corrected
  Error.lambdas <- error.lambdas+optimism
  
  #return list of interesting variables.
  list(lambdas=lambdas,Error.lambdas=Error.lambdas,error.lambdas=error.lambdas,
       optimism=optimism,fits=fits,predictions=predictions,
       predicted.fit=predicted.fit,evaluation.dat=evaluation.dat)
}
```
At the end of this function, `optimisim` is as written in equation \eqref{optimism} and `Error.lambdas` is in equation \eqref{optcor}. 

### Corrected lambda function

This function is called by `aim2` to get the optimism correction for the prediction error. This implements the parametric boostrap and evaluates equations \eqref{sigma_z1_calM} - \eqref{eq: barzstar} and returns the righthand side of \eqref{optcor}.

**This function assumes that the last column of the data is the outcome -- this needs to be fixed**
**Two places: dat[,p] and new.dat[,p] <- boot.dat[,b]**
```{r}
corrected.lambda <- function(dat,lambdas,list.object,model,predicted.values,alphas,n.boot=10)
  {
    n1 <- nrow(dat)
    p <- ncol(dat)
    n.lambdas <- length(lambdas)
    cilambda <- matrix(0,n1,n.lambdas)
    boot.dat <- boot.residual <- matrix(NA,n1,n.boot)
    sigmahat <- sqrt(sum((dat[,p]-predicted.values)^2)/n1) #SD for \hat{\sigma^2_{Z_1-\bigM}}

    for(b in 1:n.boot) boot.dat[,b] <- rnorm(n1,mean=predicted.values,sd=sigmahat)#bootstrap samples
    boot.mean <- matrix(apply(boot.dat,1,mean))  # \bar{Z^*_{1i}}
    for(i in 1:nrow(boot.dat)) boot.residual[i,] <- boot.dat[i,]-boot.mean[i]
    for(b in 1:n.boot)
      {
        new.dat <- dat
        new.dat[,p] <- boot.dat[,b] 
        for(j in 1:n.lambdas)
          {
            final.fit <- rpart(outvar.aim2 ~ .,data = new.dat,
                               parms=list(lambda=lambdas[j],
                               yhat=predicted.values,alpha=alphas),
                               method=list.object)
            bigMhat <- predict(object=final.fit,newdata=new.dat)
            cilambda[,j] <- cilambda[,j]+bigMhat*boot.residual[,b]
          }
      }
    cilambda <- cilambda/(n.boot-1)
    return(2*apply(cilambda,2,sum))
  }
```




### Hand build `rpart` tree
To begin we call needed libraries and code the rpart functions for init, eval, and split which are specific to our algorithm.
To build an
rpart tree by hand, a list of functions needs to be fed to the rpart
call. That list is referred to as aim2.list, and used with the argument
method=aim2.list.  Important functions are an initialization function
(aim2.init), an evaluation function (aim2.eval), and a splitting
function (aim2.split).  Note that in aim2.init the $y$ variable
contains three columns: the evaluation set outcome variables $Z_{1i}$, 
the $\alpha$'s,
and the predicted values $\widehat{Z_{1i}}$.  In aim2.eval the value
of \eqref{bigopt} is computed for the chosen split.  In aim2.split the
optimal split is found.  This is done currently by looping through
every value of every variable.  Future effort will be undertaken to
see if the loop can be removed.  
```{r}


#y contains response Z_i, Zhat_i from RF, and alpha where alpha=1/var(zhat)

aim2.init <- function(y, offset, parms, wt)
{
  if (!is.null(offset)) y[,1] <- y[,1]-offset
  list(y=cbind(y,parms$yhat,parms$alpha),parms=parms, numy=3, numresp=1, summary=aim2.summary)
}

aim2.eval <- function(y, wt, parms)
{
  n <- length(y)/3
  lambda <- parms$lambda
  yhat <- y[,2]
  alphas <- y[,3]
  alphabar <- sum(alphas)/n
  y1 <- y[,1]
  r <- 1/(1+lambda*alphabar)
  zbar <- mean(y1)
  zbarhat <- sum(yhat*alphas)/sum(alphas)
  chat <- r*zbar+(1-r)*zbarhat
  rss <- sum((y1-chat)^2+lambda*alphas*(chat-yhat)^2)
  list(label=chat, deviance=rss)
}

aim2.split <- function(y, wt, x, parms, continuous)
  {
    n <- length(y[,1])
    y1 <- y[,1]
    yhat <- y[,2]
    alpha <- y[,3]
    lambda <- parms$lambda
    if (continuous)
      {
        if(is.null(lambda)) compute.lambda #Placeholder until I figure out how to compute lambda
        goodness <- direction <- double(n-1) #Allocate 0 vector
        y.cumsum <- cumsum(y1)
        y.left <- y.cumsum[-n]
        y.right <- y.cumsum[n]-y.left
        yhat.cumsum <- cumsum(yhat*alpha)
        yhat.left <- yhat.cumsum[-n]
        yhat.right <- yhat.cumsum[n]-yhat.left
        alpha.cumsum <- cumsum(alpha)
        alpha.left <- alpha.cumsum[-n]
        alpha.right <- alpha.cumsum[n]-alpha.left
        for(i in 1:(n-1))
          {
            zbar.left <- y.left[i]/i
            zbar.right <- y.right[i]/(n-i)
            zbarhat.left <- yhat.left[i]/alpha.left[i]
            zbarhat.right <- yhat.right[i]/alpha.right[i]
            alphabar.left <- alpha.left[i]/i
            alphabar.right <- alpha.right[i]/(n-i)
            r.left <- 1/(1+lambda*alphabar.left)
            r.right <- 1/(1+lambda*alphabar.right)
            chat.left <- r.left*zbar.left+(1-r.left)*zbarhat.left
            chat.right <- r.right*zbar.right+(1-r.right)*zbarhat.right

#            goodness[i] <- sum((y1-mean(y1))^2) 
#                 - (sum((y1[1:i]-chat.left)^2 +
#                    lambda*alpha[1:i]*(yhat[1:i]-chat.left)^2) +
#                   sum((y1[(i+1):n]-chat.right)^2 +
#                  lambda*alpha[(i+1):n]*(yhat[(i+1):n]-chat.right)^2)) 
#           Do we need adjustment for missing values like in  vignette example?
            
            direction[i] <- sign(zbar.left-zbar.right)
            goodness.left <- sum((y1[1:i]-chat.left)^2 + lambda*alpha[1:i]*(yhat[1:i]-chat.left)^2)
            goodness.right <- sum((y1[(i+1):n]-chat.right)^2 + 
                                    lambda*alpha[(i+1):n]*(yhat[(i+1):n]-chat.right)^2)
            tss <- sum((y1-mean(y1))^2)
            goodness[i] <- tss-goodness.left-goodness.right

          }
      } # this means we can only have x continuous - no categorical
#    goodness <- 1/goodness
    return(list(goodness=goodness, direction=direction))
  }

aim2.summary <- function(yval, dev, wt, ylevel, digits )
{
  paste(" mean=", format(signif(yval, digits)), ", MSE=" , format(signif(dev/wt, digits)), sep= '')
}

aim2.text <- function(yval, dev, wt, ylevel, digits, n, use.n )
{
  if(use.n) paste(formatg(yval,digits)," nn=", n,sep="")
  else paste(formatg(yval,digits))
}



```

## Example: NABTC data
For an example, we have the NABTC data. 

\textit{From Survival Paper:} data from 12 North American Brain Tumor Consortium (NABTC)
Phase II clinical trials for recurrent glioma \citep{Wu01022010}, run to assess the
efficacy  of novel therapeutic agents in patients with grade III or IV gliomas.
As there are few efficacious treatments for high-grade glioma and median survival is low
\citep[14.6 months;][]{doi:10.1056/NEJMoa043330}, there is strong interest in
identifying prognostic factors associated with overall survival.
Recently, several studies have combined data from multiple (necessarily small)
Phase II clinical trials to examine such factors using a various statistical methods; however,
findings have at best been moderately consistent \citep{Wong01081999, Carson20062007, Wu01022010}.


\citet{Wu01022010} combined the NABTC data with 15 North Central Cancer Treatment Group (NCCTG) trials and found tumor grade, patient age, baseline performance score, and time since diagnosis as important prognostic variables. Based on these results and those of earlier studies,
\citet{Wu01022010} conclude that there is strong evidence to suggest
that future trials should collect and report this information
as predictors of patient prognosis. Limitations of these analyses, and those to be presented below, include the possibility of trial referral bias and biases induced as a result of variation in patient eligibility criteria across studies.


The NABTC data set analyzed here includes 549 patients that were treated on Phase II trials between February 1998 and December 2002 \citep{Wu01022010}; Web Table 7 summarizes the data on 18 variables that include age, gender and several variables documenting a patient's health and tumor status as well as current/past therapies.
The outcome of interest is overall survival (OS), defined as time from study registration date to the date of death due to any cause (median OS was 30.4 weeks). Thirty patients were censored at last follow-up date, being still alive or lost to follow-up.
There are several noteworthy differences between the analysis and results here and in \cite{Wu01022010}. First, many of the results in that paper refer to the combined analysis of 27 trials, not just the 12 trials considered here; in addition, and important to the motivation for our analysis, is the fact that the risk groups ultimately identified in \citet[Table 5]{Wu01022010} were determined
by post-processing the results of their analyses in a subjective and relatively ad hoc manner.
In particular, their primary analysis consisted of estimating the cumulative hazard function
for OS using a Cox proportional hazard model adjusted only for current temozolomide (TMZ) use
and then subsequently analyzing this predicted count using CART without any further accounting
for censoring. In subsequent analysis, logrank tests were used to test for pairwise differences
between terminal nodes; terminal nodes were then combined for the purposes of defining risk groups if the p-value  from a corresponding log-rank test was $>0.01$ \citep[][Table 5]{Wu01022010}.  That is, \cite{Wu01022010} form risk groups
using a combination of 'and' statements derived from CART and 'or' statements
created using the indicated testing procedure. Important differences between
their approach and \emph{partDSA} include an improper accounting for censoring,
a lack of objectivity in the methodology used for defining risk groups,
and the failure to use cross-validation in evaluating the final models.


In the following block of text we read in the data, rename the columns, run the algorithm (via `aim2` function) and then plot the errors. 

```{r echo=FALSE, message=FALSE}
library(randomForest)
library(rpart)
library(rpart.plot)
```
```{r}
dat <- read.table("/Users/annettemolinaro/Box Sync/Yale/Current Projects/Survival Paper/Paper Jan 2011/data_6_11/nabtc/SendRob.txt",
                    sep="\t",header=TRUE,as.is=FALSE)
#519 have events, 30 are censored.
dat3 <- dat2 <- dat[which(dat$cens==1),]

dat3$OS <- log(dat3$OS)

dat4 <- dat3[,-which(colnames(dat3)=="academic"|colnames(dat3)=="cens")]
dat4$pchemo[which(dat4$pchemo=="")] <- NA

dat5 <- dat4[,is.na(match(colnames(dat4),c("dxtime","extpres","prelapse","lowgr")))]
dat5$pchemo[is.na(dat5$pchemo)] <- "Yes"
dat5$KarnofskyPS[is.na(dat5$KarnofskyPS)] <- 80
dat5$lhist<-as.numeric(dat5$lhist)
dat5$anticon<-as.numeric(dat5$anticon)
dat5$pchemo<-as.numeric(dat5$pchemo)
dat5$pnitro<-as.numeric(dat5$pnitro)
dat5$tmz<-as.numeric(dat5$tmz)

colnames(dat5)[[1]]<-"mdev"
temp <- aim2(dat=dat5,nreps=1,n.grid=20,mult=2,seed=12345,outvar="mdev")

plot(temp$lambdas,temp$Error.lambdas,xlab="lambda",ylab="Optimism corrected error",
       main="Optimism corrected error vs. lambda for NABTC data")
```

According to this plot, we would select the lambda equal to 
`r temp$lambdas[which(temp$Error.lambdas==min(temp$Error.lambdas))]` is the one which minimizes the optimism corrected error. Using that lambda we get the following `rpart` tree:

```{r echo=FALSE, message = FALSE}
  lambda.num<-which(temp$Error.lambdas==min(temp$Error.lambdas))
  prp(temp$fits[[lambda.num]])
```


If we just ran a rpart tree:

```{r}

summary(justRpart<-rpart(mdev ~ .,data = dat5)  )
justRpartpruned<-prune(justRpart,cp=0.03)
prp(justRpartpruned)
```


