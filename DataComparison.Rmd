---
title: "Data Example with Kidney Data"
author: "A, A, & R"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  html_document: default
  pdf_document: default
---

```{r echo=FALSE}
library(knitr)
read_chunk("/Users/annettemolinaro/Respository/Aim2/IntegratingCodeAndText_postAMM.R")
```


```{r }
<<klibraries>>
```


```{r}
 <<kaim2>>
```


```{r}
<<kcorrected.lambda>> 
```

```{r}
<<krpartfxs>>

```

## Example:  data
For an example, we have the gen.data data. In the following block of text we read in the data, rename the columns, run the algorithm (via `aim2` function) and then plot the errors. 

```{r}
############ Fill in data info ####################

dat <- read.csv("SaratogaHouses.csv",header=TRUE,row.names=1)

#> colnames(dat)
#[1] "price"           "lotSize"         "age"             "landValue"      
# [5] "livingArea"      "pctCollege"      "bedrooms"        "fireplaces"     
# [9] "bathrooms"       "rooms"           "heating"         "fuel"           
# [13] "sewer"           "waterfront"      "newConstruction" "centralAir"     

#Take square root of price

sdat <- dat
sdat$price <- sqrt(dat$price)

cor.price <- rep(NA,14)
for(i in 1:14) cor.price
    
fit <- rpart(price ~ .,data=sdat)
prune.fit <- prune(fit,cp=0.01)

fit2 <- randomForest(price ~ .,data=sdat)

n <- nrow(sdat)
samp <- sample(1:n,replace=FALSE)
n.test <- round(n/3)
n.train <- n-n.test
samp.train <- sort(samp[1:n.train])
samp.test <- sort(samp[(n.train+1):n])

sdat.train <- sdat[samp.train,]
sdat.test <- sdat[samp.test,]

fit.train <- rpart(price ~ .,data=sdat.train)
prune.train <- prune(fit.train,cp=0.018)
fit2.train <- randomForest(price ~ .,data=sdat.train)

pdf("RpartHousing.pdf")
prp(prune.train)
dev.off()


predict.test <- predict(prune.train,newdata=sdat.test)
predict2.test <- predict(fit2.train,newdata=sdat.test)

res.test <- sdat.test$price-predict.test
res2.test <- sdat.test$price-predict2.test

par(mfrow=c(2,1))
hist(res.test)
hist(res2.test)

error.test <- sqrt(sum(res.test^2)/n.test)
error2.test <- sqrt(sum(res2.test^2)/n.test)
fit3 <- composite.rpart.thirds(dat=sdat.train,n.grid=100,mult=3,uplim = 0,outvar="price",verbose=FALSE)
predict3.test <- predict(fit3$current.fit.pruned,newdata=sdat.test)

res3.test <- sdat.test$price-predict3.test
error3.test <- sqrt(sum(res3.test^2)/n.test)

pdf("ThirdsHousing.pdf")
prp(fit3$current.fit.pruned)
dev.off()


#########################################################






set.seed(12345)
gen.data <- dat.complete

temp <- aim2(dat=gen.data,nreps=1,n.grid=20,mult=2,outvar="mdev",prop.learning=0.5)

  plot(temp$lambdas,temp$Error.lambdas,xlab="lambda",ylab="Optimism corrected error",
       main="Optimism corrected error vs. lambda for housing data")
```

According to this plot, we would select the lambda equal to 
$`r temp$lambdas[which(temp$Error.lambdas==min(temp$Error.lambdas))]`$ is the one which minimizes the optimism corrected error. Using that lambda we get the following rpart tree:

```{r echo=FALSE, message = FALSE}
  lambda.num<-which(temp$Error.lambdas==min(temp$Error.lambdas))
  prp(temp$fits[[ lambda.num]])
   test.dat.predicted.fit.resub<-predict(object=temp$fits[[lambda.num]],newdata=gen.data)
  error.test.dat.resub <- sum((gen.data$mdev-test.dat.predicted.fit.resub)^2)
```


The resubstitution estimate of prediction error is $ `r  round(error.test.dat.resub,3)`$.   


If we just ran a rpart tree:

```{r}

(justRpart<-rpart(mdev ~ .,data = gen.data)  )
#prp(justRpart)
min.CP<-justRpart$cptable[which(justRpart$cptable[,4]==min(justRpart$cptable[,4])),1]
justRpartpruned<-prune(justRpart,cp=min.CP)
prp(justRpartpruned)
   rpart.test.dat.predicted.fit.resub<-predict(object=justRpartpruned,newdata=gen.data)
  rpart.error.test.dat.resub <- sum((gen.data$mdev-rpart.test.dat.predicted.fit.resub)^2)
```

With prediction error $`r round(rpart.error.test.dat.resub,3)`$


## Switching to Composite functions.
At the moment we have three composite functions - the original we worked on in DC, the "thirds" version Adam has worked on and Rob's version which searches a grid and uses a logspline density for the bootstrap. 



### DC Code 

```{r}
<<kcomposite>>

```

The  data with the DC Code looks like:


```{r}
temp <- composite.rpart(dat=gen.data,n.grid=50,mult=4,outvar="mdev")
plot(temp$lambdas,temp$error.lambdas,xlab="lambda",ylab="Optimism corrected error",
     main="DC Code: Optimism corrected error vs. lambda for housing data")
```
The min of which is$ `r temp$lambdas[temp$error.lambdas==min(temp$error.lambdas)]`$. 

The corresponding tree is: 
```{r echo=FALSE, message = FALSE}
  lambda.num<-which(temp$error.lambdas==min(temp$error.lambdas))
  prp(temp$fits[[ lambda.num]])
   test.dat.predicted.fit.resub<-predict(object=temp$fits[[lambda.num]],newdata=gen.data)
  error.test.dat.resub <- sum((gen.data$mdev-test.dat.predicted.fit.resub)^2)
```

With corresponding resubstitution estimate of $`r error.test.dat.resub`$. 


### Third's Code 

```{r}
<<kcomposite.thirds>>
```



```{r}
temp2 <- composite.rpart.thirds(dat=gen.data,n.grid=100,mult=4,outvar="mdev")

  #plot(temp2$lambdas,temp2$error.lambdas,xlab="lambda",ylab="Optimism corrected error",
  #     main="Thirds: Optimism corrected error vs. lambda for housing data")
```


The corresponding tree is: 
```{r echo=FALSE, message = FALSE}
  prp(temp2$current.fit.pruned)
   test.dat.predicted.fit.resub<-predict(temp2$current.fit.pruned,newdata=gen.data)
  error.test.dat.resub <- sum((gen.data$mdev-test.dat.predicted.fit.resub)^2)
```

With corresponding resubstitution estimate of $`r error.test.dat.resub`$. 


### Rob's Code 
Comments from Rob:
  
  
  One modification that I did make is to use a log-spline density estimator in place of a normal distribution for carrying out the parametric bootstrap; you need the package logspline from CRAN. This does quite a good job in capturing and then subsequently simulating data with a response profile that looks similar to the original (definitely better than the normal). But unfortunately the cross-validation curve that gets generated doesn’t really differ a lot from that obtained using the normal distribution. Not really sure what is going on here.

The leave one out CV procedure has a clearly defined minimum for lambda for the housing data, at least over the range of lambdas I considered. The parametric bootstrap gives a local min that is in the same ballpark.  I haven’t looked at the corresponding composite response tree built using lambdas in this range. 

If you run it, it will take a while (3-5 mins) since it does both LOO and parametric bootstrap CV (with 1000 replicates – 10 is way too small).

Two other things:
  
  * I don’t think the original method for calculating the “root node” lambda is correct for the composite methods so right now we are stuck using a grid.
* The composite response seems to have some potential to create heteroscedasticity – I think low variance predictions will generate composite responses with smaller variances compared to those with high variance predictions, for example. There has been some work on studying the performance of trees in this setting but I don’t know what kind of software might be out there to fit such trees.



```{r}
<<RobsComposite>>
  ```

```{r}
temp3 <- composite.rpart.Grid(dat = gen.data, n.grid = 20, mult = 1, 
                              uplim = 20, outvar = "mdev", prop.learning = 0.5)


plot(temp3$lambdas, temp3$errorU.lambdas, xlab = "lambda", 
     ylab = "apparent error", main = "apparent errorU (orig response) vs. lambda")
plot(temp3$lambdas, temp3$Error.lambdas, xlab = "lambda", ylab = "Corrected error", 
     main = "op-corr apparent errorU vs. lambda")
plot(temp3$lambdas, temp3$error.lambdas, xlab = "lambda", ylab = "apparent error", 
     main = "apparent error vs. lambda")
plot(temp3$lambdas, temp3$CVError.lambdas, xlab = "lambda", 
     ylab = "CV error", main = "CV-based error vs. lambda")


```

The corresponding tree is with CVError is: 
  ```{r echo=FALSE, message = FALSE}
lambda.num<-which(temp3$CVError.lambdas==min(temp3$CVError.lambdas))
prp(temp3$fits[[ lambda.num]])
test.dat.predicted.fit.resub<-predict(object=temp3$fits[[lambda.num]],newdata=gen.data)
error.test.dat.resub <- sum((gen.data$mdev-test.dat.predicted.fit.resub)^2)
```


With corresponding resubstitution estimate of $`r error.test.dat.resub`$. 



The corresponding tree is with Error + Optimism is: 
  ```{r echo=FALSE, message = FALSE}
lambda.num<-which(temp3$Error.lambdas==min(temp3$Error.lambdas))
prp(temp3$fits[[ lambda.num]])
test.dat.predicted.fit.resub<-predict(object=temp3$fits[[lambda.num]],newdata=gen.data)
error.test.dat.resub <- sum((gen.data$mdev-test.dat.predicted.fit.resub)^2)
```


With corresponding resubstitution estimate of $`r error.test.dat.resub`$. 
